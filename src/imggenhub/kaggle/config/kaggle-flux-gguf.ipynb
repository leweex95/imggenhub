{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81590045",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9bc8d5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------- SETTINGS ---------\n",
        "# Choose model source: \"dataset\" or \"huggingface\"\n",
        "MODEL_SOURCE = os.getenv(\"MODEL_SOURCE\", \"huggingface\")  # dataset or huggingface\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
        "\n",
        "if MODEL_SOURCE == \"dataset\":\n",
        "    # Model paths from Kaggle datasets\n",
        "    DIFFUSION_MODEL = \"/kaggle/input/flux1-schnell-q4-zip/flux1-schnell-Q4_0.gguf\"\n",
        "    VAE_MODEL = \"/kaggle/input/vae-zip/ae.safetensors\"\n",
        "    CLIP_L_MODEL = \"/kaggle/input/clip-l-zip/clip_l.safetensors\"\n",
        "    T5XXL_MODEL = \"/kaggle/input/t5xxl-zip/t5xxl_fp8_e4m3fn.safetensors\"\n",
        "else:\n",
        "    # Download from HuggingFace\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    print(\"Downloading models from HuggingFace...\")\n",
        "    \n",
        "    DIFFUSION_MODEL = hf_hub_download(\n",
        "        repo_id=\"city96/FLUX.1-schnell-gguf\",\n",
        "        filename=\"flux1-schnell-Q4_0.gguf\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    VAE_MODEL = hf_hub_download(\n",
        "        repo_id=\"black-forest-labs/FLUX.1-schnell\",\n",
        "        filename=\"ae.safetensors\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    CLIP_L_MODEL = hf_hub_download(\n",
        "        repo_id=\"comfyanonymous/flux_text_encoders\",\n",
        "        filename=\"clip_l.safetensors\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    T5XXL_MODEL = hf_hub_download(\n",
        "        repo_id=\"comfyanonymous/flux_text_encoders\",\n",
        "        filename=\"t5xxl_fp8_e4m3fn.safetensors\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    print(\"Models downloaded successfully!\")\n",
        "\n",
        "# -------- PARAMETERS (can be overridden when running via papermill) ---------\n",
        "PROMPTS = ['A beautiful landscape']\n",
        "NEGATIVE_PROMPT = \"\"\n",
        "OUTPUT_DIR = \"images\"\n",
        "IMG_SIZE = (112, 112)\n",
        "GUIDANCE = 3.5\n",
        "STEPS = 4\n",
        "SEED = 42\n",
        "PRECISION = \"q4\"\n",
        "# ----------------------------\n",
        "\n",
        "print(f\"Device: GPU (FLUX GGUF Q4)\")\n",
        "print(f\"Model Source: {MODEL_SOURCE}\")\n",
        "print(f\"\\nPrompts: {len(PROMPTS)}\")\n",
        "print(f\"Precision: {PRECISION}\")\n",
        "print(f\"Steps: {STEPS}\")\n",
        "print(f\"Image size: {IMG_SIZE}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220f3d0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup stable-diffusion.cpp executable\n",
        "!mkdir -p /kaggle/working/stable-diffusion.cpp\n",
        "!cp -r /kaggle/input/sd-build-zip/kaggle/working/export/sd_build/* /kaggle/working/stable-diffusion.cpp/\n",
        "!chmod +x /kaggle/working/stable-diffusion.cpp/build/bin/sd\n",
        "\n",
        "sd_executable = \"/kaggle/working/stable-diffusion.cpp/build/bin/sd\"\n",
        "if not os.path.isfile(sd_executable):\n",
        "    raise FileNotFoundError(\"sd executable not found at: \" + sd_executable)\n",
        "\n",
        "print(f\"SD executable ready: {sd_executable}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca81260e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images for each prompt\n",
        "print(\"-\"*60)\n",
        "print(f\"Model paths:\")\n",
        "print(f\"  Diffusion: {DIFFUSION_MODEL}\")\n",
        "print(f\"  VAE: {VAE_MODEL}\")\n",
        "print(f\"  CLIP-L: {CLIP_L_MODEL}\")\n",
        "print(f\"  T5XXL: {T5XXL_MODEL}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for i, prompt in enumerate(tqdm(PROMPTS, desc=\"Generating\")):\n",
        "    print(\"-\"*60)\n",
        "    print(f\"Prompt {i+1}/{len(PROMPTS)}: {prompt[:60]}...\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # Generate unique filename with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"generated_{i+1}_{timestamp}.png\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, filename)\n",
        "    \n",
        "    cmd = [\n",
        "        sd_executable,\n",
        "        \"--diffusion-model\", DIFFUSION_MODEL,\n",
        "        \"--vae\", VAE_MODEL,\n",
        "        \"--clip_l\", CLIP_L_MODEL,\n",
        "        \"--t5xxl\", T5XXL_MODEL,\n",
        "        \"--prompt\", prompt,\n",
        "        \"--cfg-scale\", str(GUIDANCE),\n",
        "        \"--sampling-method\", \"euler\",\n",
        "        \"--steps\", str(STEPS),\n",
        "        \"--width\", str(IMG_SIZE[1]),\n",
        "        \"--height\", str(IMG_SIZE[0]),\n",
        "        \"--seed\", str(SEED),\n",
        "        \"--output\", output_path,\n",
        "        \"--rng\", \"cuda\",\n",
        "        \"-v\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"Running generation...\")\n",
        "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    \n",
        "    print(f\"Return code: {res.returncode}\")\n",
        "    if res.returncode == 0:\n",
        "        print(f\"Saved: {output_path}\")\n",
        "    else:\n",
        "        print(f\"ERROR: Generation failed\")\n",
        "        print(f\"STDOUT: {res.stdout}\")\n",
        "        print(f\"STDERR: {res.stderr}\")\n",
        "    \n",
        "    # Cleanup\n",
        "    gc.collect()\n",
        "\n",
        "print(\"-\"*60)\n",
        "print(f\"COMPLETE! {len(PROMPTS)} images saved to {OUTPUT_DIR}\")\n",
        "print(\"-\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
