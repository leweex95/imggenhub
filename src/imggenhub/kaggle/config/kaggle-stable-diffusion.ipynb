{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c87919",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
        "os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\n",
        "os.environ.setdefault(\"HF_HUB_DISABLE_PROGRESS_BARS\", \"1\")\n",
        "os.environ.setdefault(\"HF_HUB_DOWNLOAD_TIMEOUT\", \"1800\")\n",
        "\n",
        "HF_TOKEN_PATH = \"/kaggle/input/imggenhub-hf-token/hf_token.json\"\n",
        "with open(HF_TOKEN_PATH, \"r\", encoding=\"utf-8\") as hf_file:\n",
        "    HF_TOKEN = json.load(hf_file)[\"HF_TOKEN\"]\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815b8b76",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise RuntimeError(\"HF_TOKEN not found. Ensure the Kaggle dataset is attached and synced.\")\n",
        "\n",
        "# -------- SETTINGS ---------\n",
        "MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "REFINER_MODEL_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "\n",
        "# -------- PARAMETERS (can be overridden when running via papermill) ---------\n",
        "PROMPTS = ['Fresh test: stable diffusion']\n",
        "NEGATIVE_PROMPT = \"blurry, distorted\"\n",
        "OUTPUT_DIR = \".\"\n",
        "IMG_SIZE = (64, 64)\n",
        "GUIDANCE = 0.8\n",
        "PRECISION = \"fp32\"\n",
        "STEPS = 1\n",
        "SEED = 42\n",
        "USE_GPU = True\n",
        "USE_REFINER = True\n",
        "REFINER_STEPS = 25\n",
        "REFINER_GUIDANCE = 7.0\n",
        "REFINER_PRECISION = \"fp16\"\n",
        "# ----------------------------\n",
        "\n",
        "def get_vram_gb():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**3\n",
        "    return 0.0\n",
        "\n",
        "device = \"cuda\" if USE_GPU and torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(f\"\\nVRAM at start: {get_vram_gb():.2f} GB\")\n",
        "print(f\"Prompts: {len(PROMPTS)}\")\n",
        "print(f\"Base precision: {PRECISION}, Refiner precision: {REFINER_PRECISION}\")\n",
        "print(f\"Steps: {STEPS} (base), {REFINER_STEPS} (refiner)\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Determine base model precision\n",
        "if PRECISION == \"fp16\":\n",
        "    base_dtype = torch.float16\n",
        "    base_variant = \"fp16\"\n",
        "elif PRECISION == \"fp32\":\n",
        "    base_dtype = torch.float32\n",
        "    base_variant = None\n",
        "else:\n",
        "    base_dtype = torch.float16\n",
        "    base_variant = \"fp16\"\n",
        "\n",
        "# Determine refiner precision\n",
        "if REFINER_PRECISION == \"fp16\":\n",
        "    refiner_dtype = torch.float16\n",
        "    refiner_variant = \"fp16\"\n",
        "elif REFINER_PRECISION == \"fp32\":\n",
        "    refiner_dtype = torch.float32\n",
        "    refiner_variant = None\n",
        "else:\n",
        "    refiner_dtype = torch.float16\n",
        "    refiner_variant = \"fp16\"\n",
        "\n",
        "generator = torch.manual_seed(SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# SEQUENTIAL TWO-STAGE GENERATION\n",
        "# ============================================================================\n",
        "for i, prompt in enumerate(tqdm(PROMPTS, desc=\"Generating\")):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Prompt {i+1}/{len(PROMPTS)}: {prompt[:60]}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STAGE 1: LOAD BASE MODEL → GENERATE → DELETE\n",
        "    # ========================================================================\n",
        "    print(f\"\\n[STAGE 1: BASE MODEL]\")\n",
        "    print(f\"  Loading base model...\")\n",
        "    print(f\"  VRAM before load: {get_vram_gb():.2f} GB\")\n",
        "    \n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=base_dtype,\n",
        "        variant=base_variant,\n",
        "        use_safetensors=True,\n",
        "        token=HF_TOKEN,\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"  VRAM after load: {get_vram_gb():.2f} GB\")\n",
        "    print(f\"  Running base inference ({STEPS} steps)...\")\n",
        "    \n",
        "    base_image = pipe(\n",
        "        prompt,\n",
        "        negative_prompt=NEGATIVE_PROMPT,\n",
        "        guidance_scale=GUIDANCE,\n",
        "        num_inference_steps=STEPS,\n",
        "        generator=generator,\n",
        "        height=IMG_SIZE[0],\n",
        "        width=IMG_SIZE[1],\n",
        "        denoising_end=0.8 if USE_REFINER else None,\n",
        "    ).images[0]\n",
        "    \n",
        "    print(f\"  VRAM after inference: {get_vram_gb():.2f} GB\")\n",
        "    print(f\"  Deleting base model from memory...\")\n",
        "    \n",
        "    del pipe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f\"  VRAM after cleanup: {get_vram_gb():.2f} GB\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STAGE 2: LOAD REFINER → REFINE → DELETE\n",
        "    # ========================================================================\n",
        "    if USE_REFINER:\n",
        "        print(f\"\\n[STAGE 2: REFINER MODEL]\")\n",
        "        print(f\"  Loading refiner...\")\n",
        "        print(f\"  VRAM before load: {get_vram_gb():.2f} GB\")\n",
        "        \n",
        "        refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "            REFINER_MODEL_ID,\n",
        "            torch_dtype=refiner_dtype,\n",
        "            variant=refiner_variant,\n",
        "            use_safetensors=True,\n",
        "            token=HF_TOKEN,\n",
        "        ).to(device)\n",
        "        \n",
        "        print(f\"  VRAM after load: {get_vram_gb():.2f} GB\")\n",
        "        print(f\"  Running refiner inference ({REFINER_STEPS} steps)...\")\n",
        "        \n",
        "        final_image = refiner(\n",
        "            prompt,\n",
        "            negative_prompt=NEGATIVE_PROMPT,\n",
        "            image=base_image,\n",
        "            guidance_scale=REFINER_GUIDANCE,\n",
        "            num_inference_steps=REFINER_STEPS,\n",
        "            generator=generator,\n",
        "            denoising_start=0.8,\n",
        "        ).images[0]\n",
        "        \n",
        "        print(f\"  VRAM after inference: {get_vram_gb():.2f} GB\")\n",
        "        print(f\"  Deleting refiner from memory...\")\n",
        "        \n",
        "        del refiner\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        print(f\"  VRAM after cleanup: {get_vram_gb():.2f} GB\")\n",
        "    else:\n",
        "        final_image = base_image\n",
        "    \n",
        "    # Save\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"generated_{i+1}_{timestamp}.png\"\n",
        "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
        "    final_image.save(filepath)\n",
        "    print(f\"\\nSaved: {filepath}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"COMPLETE! {len(PROMPTS)} images saved to {OUTPUT_DIR}\")\n",
        "print(f\"Final VRAM: {get_vram_gb():.2f} GB\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
