{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a569b2cb",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import gc \n",
        "import os\n",
        "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b46c5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import FluxPipeline\n",
        "from datetime import datetime\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e59a589",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# -------- LOAD HF TOKEN FROM DATASET ---------\n",
        "HF_TOKEN_PATH = \"/kaggle/input/imggenhub-hf-token/hf_token.json\"\n",
        "with open(HF_TOKEN_PATH, \"r\") as f:\n",
        "    HF_TOKEN = json.load(f)[\"HF_TOKEN\"]\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "print(\"HF_TOKEN loaded from dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc79834",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"\n",
        "PROMPTS = ['Fresh flux bf16 test']\n",
        "OUTPUT_DIR = \".\"\n",
        "IMG_SIZE = (1024, 1024)\n",
        "GUIDANCE = 0.0\n",
        "STEPS = 4\n",
        "SEED = 42\n",
        "PRECISION = \"bf16\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23f73b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "dtype_map = {\"bf16\": torch.bfloat16, \"fp16\": torch.float16, \"fp32\": torch.float32}\n",
        "torch_dtype = dtype_map.get(PRECISION, torch.bfloat16)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "def get_vram_gb():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**3\n",
        "    return 0.0\n",
        "\n",
        "print(f\"Loading model...\")\n",
        "pipe = FluxPipeline.from_pretrained(MODEL_ID, torch_dtype=torch_dtype, token=HF_TOKEN)\n",
        "pipe.enable_vae_tiling()\n",
        "pipe.enable_attention_slicing()\n",
        "pipe.set_progress_bar_config(disable=False)\n",
        "pipe.enable_sequential_cpu_offload()\n",
        "\n",
        "print(f\"Model loaded (VRAM: {get_vram_gb():.2f} GB)\")\n",
        "\n",
        "for i, prompt in enumerate(PROMPTS):\n",
        "    print(f\"[{i+1}/{len(PROMPTS)}] {prompt}\")\n",
        "    generator = torch.Generator(device=device).manual_seed(SEED + i)\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        height=IMG_SIZE[0],\n",
        "        width=IMG_SIZE[1],\n",
        "        guidance_scale=GUIDANCE,\n",
        "        num_inference_steps=STEPS,\n",
        "        generator=generator,\n",
        "        max_sequence_length=256,\n",
        "    ).images[0]\n",
        "    filename = f\"flux_{i+1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "    image.save(os.path.join(OUTPUT_DIR, filename))\n",
        "    print(f\"Saved: {filename}\")\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "print(f\"Complete! {len(PROMPTS)} images in {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d382dcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "from PIL import Image\n",
        "import glob\n",
        "import natsort\n",
        "\n",
        "# Collect all generated PNGs\n",
        "image_paths = natsort.natsorted(glob.glob(os.path.join(OUTPUT_DIR, \"generated_*.png\")))\n",
        "\n",
        "print(f\"Displaying {len(image_paths)} generated images with prompts:\")\n",
        "\n",
        "# Make sure PROMPTS order matches generated images\n",
        "for i, path in enumerate(image_paths):\n",
        "    prompt = PROMPTS[i] if i < len(PROMPTS) else \"Unknown prompt\"\n",
        "    display(Markdown(f\"**Prompt {i+1}:** {prompt}\"))\n",
        "    img = Image.open(path)\n",
        "    display(img)\n",
        "    print(\"-\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}