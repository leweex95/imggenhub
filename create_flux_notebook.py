import json

notebook = {
    "cells": [
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -U diffusers transformers accelerate torch safetensors omegaconf hf_xet kagglehub peft>=0.17.0 pillow opencv-python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import gc\n",
                "from diffusers import FluxPipeline\n",
                "from tqdm import tqdm\n",
                "from datetime import datetime\n",
                "from transformers import CLIPImageProcessor\n",
                "\n",
                "# -------- SETTINGS ---------\n",
                "# Using official FLUX.1-schnell with fp16 precision for Kaggle compatibility\n",
                "# Tried mflux-4bit but it had model loading issues on Kaggle\n",
                "MODEL_ID = \"black-forest-labs/FLUX.1-schnell\"\n",
                "\n",
                "# -------- PARAMETERS (can be overridden when running via papermill) ---------\n",
                "PROMPTS = ['photorealistic bombed out burnt high soviet apartment block kharkiv ukrainian war']\n",
                "OUTPUT_DIR = \"output_images_flux_official\"\n",
                "IMG_SIZE = (1080, 1920)\n",
                "GUIDANCE = 0.0  # FLUX works best with 0.0 guidance\n",
                "STEPS = 4  # FLUX schnell needs only 4 steps\n",
                "SEED = 42\n",
                "USE_GPU = True\n",
                "# ----------------------------\n",
                "\n",
                "def get_vram_gb():\n",
                "    if torch.cuda.is_available():\n",
                "        return torch.cuda.memory_allocated() / 1024**3\n",
                "    return 0.0\n",
                "\n",
                "device = \"cuda\" if USE_GPU and torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Device: {device}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "print(f\"\\\\nVRAM at start: {get_vram_gb():.2f} GB\")\n",
                "print(f\"Prompts: {len(PROMPTS)}\")\n",
                "print(f\"Model: {MODEL_ID}\")\n",
                "print(f\"Precision: bfloat16 (standard Flux inference dtype)\")\n",
                "print(f\"Steps: {STEPS}\")\n",
                "print(f\"Guidance: {GUIDANCE}\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Load Flux pipeline with bfloat16 precision for memory efficiency\n",
                "print(f\"\\\\nLoading Flux pipeline...\")\n",
                "print(f\"VRAM before load: {get_vram_gb():.2f} GB\")\n",
                "\n",
                "try:\n",
                "    pipe = FluxPipeline.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map='auto',\n",
                "    )\n",
                "except Exception as e:\n",
                "    print(f\"Error with device_map='auto', falling back to default...\")\n",
                "    pipe = FluxPipeline.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "    )\n",
                "\n",
                "pipe.enable_model_cpu_offload()\n",
                "\n",
                "print(f\"VRAM after load: {get_vram_gb():.2f} GB\")\n",
                "\n",
                "# ============================================================================\n",
                "# FLUX.1 SCHNELL GENERATION (SINGLE STAGE - NO REFINER)\n",
                "# ============================================================================\n",
                "for i, prompt in enumerate(tqdm(PROMPTS, desc=\"Generating\")):\n",
                "    print(f\"\\\\n{'='*60}\")\n",
                "    print(f\"Prompt {i+1}/{len(PROMPTS)}: {prompt[:60]}...\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    print(f\"\\\\n[FLUX.1-SCHNELL]\")\n",
                "    print(f\"  Running inference ({STEPS} steps)...\")\n",
                "    print(f\"  VRAM before inference: {get_vram_gb():.2f} GB\")\n",
                "    \n",
                "    generator = torch.Generator(device=device).manual_seed(SEED)\n",
                "    \n",
                "    image = pipe(\n",
                "        prompt,\n",
                "        height=IMG_SIZE[0],\n",
                "        width=IMG_SIZE[1],\n",
                "        guidance_scale=GUIDANCE,\n",
                "        num_inference_steps=STEPS,\n",
                "        generator=generator,\n",
                "        max_sequence_length=256,\n",
                "    ).images[0]\n",
                "    \n",
                "    print(f\"  VRAM after inference: {get_vram_gb():.2f} GB\")\n",
                "    \n",
                "    # Save image\n",
                "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    filename = f\"generated_image_{i+1}_{timestamp}.png\"\n",
                "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
                "    image.save(filepath)\n",
                "    print(f\"\\\\nâœ“ Saved: {filepath}\")\n",
                "\n",
                "print(f\"\\\\n{'='*60}\")\n",
                "print(f\"COMPLETE! {len(PROMPTS)} images saved to {OUTPUT_DIR}\")\n",
                "print(f\"Final VRAM: {get_vram_gb():.2f} GB\")\n",
                "print(f\"{'='*60}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

output_path = r'c:\Users\csibi\Desktop\imggenhub\src\imggenhub\kaggle\config\kaggle-notebook-flux-generation.ipynb'
with open(output_path, 'w') as f:
    json.dump(notebook, f, indent=2)
print(f"Notebook created at {output_path}")
